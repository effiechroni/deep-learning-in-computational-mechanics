{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ba11a8-9964-4b7e-b262-8b631b887188",
   "metadata": {},
   "source": [
    "# Exercise 19 - Extensions for physics-informed neural networks\n",
    "### Task\n",
    "A physics-informed neural network for the static bar equation is considered. Adapt the code with the following changes to enable different extensions and observe how they affect the training of the physics-informed neural network:\n",
    "- Optimzer: check block 13\n",
    "- Sampling: check block 16\n",
    "- Loss term weighting: check block 11\n",
    "    - manual weighting: check block 11\n",
    "    - automatic weighting: check blocks 11, 15\n",
    "- Strong enforcement of boundary conditions: check block 9\n",
    "- weighting per collocation point: check 10, 15\n",
    "- Learning rate scheduler: check block 17 and adapt learning rate to lr = 5e-2 in block 13\n",
    "- Activation functions: \n",
    "    - common activation functions: check blocks 13, 8\n",
    "    - learnable activation functions: check block 13\n",
    "- Numerical differentiation: check block 4\n",
    "- Convolutional neural network: check block 14, change to numerical differentiation in block 4 and remove plot of test set in block 19 (and use strong boundary enforcement of block 9 to get a convergence)\n",
    "- Feature layer: check block 6\n",
    "\n",
    "### Learning goals\n",
    "- Understand the most prominent extensions of physics-informed neural networks\n",
    "- Be able to implement the most prominent extensions of physics-informed neural networks\n",
    "- Gain an intuition on how to improve a physics-informed neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc65ef-fdb0-409a-8b58-6ee22a3a38db",
   "metadata": {},
   "source": [
    "**import libraries & set seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfeb949-ac58-4b07-9687-c41064b91f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.544248660Z",
     "start_time": "2024-01-05T14:08:13.374430725Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287a8dc-3ef0-490c-a549-a4c06474f8b9",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837a3c0-c7e6-4750-94d6-07b83721493f",
   "metadata": {},
   "source": [
    "**gradient computation with automatic differentiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ff7cc-b51d-4a65-b314-06d55506104a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.551818615Z",
     "start_time": "2024-01-05T14:08:14.545873976Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDerivativeAutomaticDifferentation(y, x, n):\n",
    "    if n == 0:\n",
    "        return y\n",
    "    else:\n",
    "        dydx = grad(\n",
    "            y, x, torch.ones(x.size()[0], 1), create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        return getDerivative(dydx, x, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e44807-ab22-454c-8a9c-c7c70e9139be",
   "metadata": {},
   "source": [
    "**gradient computation with numerical differentiation**\n",
    "\n",
    "central difference $$f'(x)\\approx\\frac{f(x+\\Delta x) - f(x-\\Delta x)}{2\\Delta x}$$\n",
    "forward difference $$f'(x)\\approx\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}$$\n",
    "backward difference $$f'(x)\\approx\\frac{f(x)-f(x-\\Delta x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e52e6-96a6-454f-980a-1803c3ec76dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.552929129Z",
     "start_time": "2024-01-05T14:08:14.549903858Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDerivativeFiniteDifference(y, x, n):\n",
    "    if n == 0:\n",
    "        return y\n",
    "    else:\n",
    "        dydx = y * 0\n",
    "        dydx[1:-1] = (y[:-2] - y[2:]) / (x[:-2] - x[2:])  # central difference\n",
    "        dydx[0] = (y[1] - y[0]) / (x[1] - x[0])  # forward difference\n",
    "        dydx[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])  # backward difference\n",
    "        return getDerivativeFiniteDifference(dydx, x, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdc50c-c6a2-4cf8-9174-0ea83594d966",
   "metadata": {},
   "source": [
    "**select differentiation method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e87dd0-5e49-4f47-b5dd-185e71d6873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.601402621Z",
     "start_time": "2024-01-05T14:08:14.552652412Z"
    }
   },
   "outputs": [],
   "source": [
    "getDerivative = getDerivativeAutomaticDifferentation\n",
    "# getDerivative = getDerivativeFiniteDifference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b0dfc-abbc-4282-ad54-1bc7ac1f74a8",
   "metadata": {},
   "source": [
    "**adaptive activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0397-8d2c-4cde-a2eb-2e2c40b4a3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.602050222Z",
     "start_time": "2024-01-05T14:08:14.598522143Z"
    }
   },
   "outputs": [],
   "source": [
    "class makeAdaptiveActivation(torch.nn.Module):\n",
    "    def __init__(self, n, activation):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.alpha = torch.nn.parameter.Parameter(torch.tensor(1.0 / n))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.n * self.alpha * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351b4bd-8953-4816-87ac-fd5abb029b75",
   "metadata": {},
   "source": [
    "**fully connected neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa82fc-8100-4a35-9709-9f3ff1272515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.602579735Z",
     "start_time": "2024-01-05T14:08:14.598620767Z"
    }
   },
   "outputs": [],
   "source": [
    "class FNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inputDimension,\n",
    "            hiddenDimensions,\n",
    "            outputDimension,\n",
    "            activationFunction=torch.nn.Tanh(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # inputDimension = 5 # hardcoded override of inputDimension for feature layer\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        modules.append(torch.nn.Linear(inputDimension, hiddenDimensions[0]))\n",
    "        modules.append(activationFunction)\n",
    "        for i in range(len(hiddenDimensions) - 1):\n",
    "            modules.append(\n",
    "                torch.nn.Linear(hiddenDimensions[i], hiddenDimensions[i + 1])\n",
    "            )\n",
    "            modules.append(activationFunction)\n",
    "        modules.append(torch.nn.Linear(hiddenDimensions[-1], outputDimension))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.cat((torch.sin(torch.pi * x),         # feature layer composed of sin(n * torch.pi * x)\n",
    "        #               torch.sin(2 * torch.pi * x),\n",
    "        #               torch.sin(3 * torch.pi * x),\n",
    "        #               torch.sin(4 * torch.pi * x),\n",
    "        #               torch.sin(5 * torch.pi * x)), 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e078fbe-ed05-4ae0-a9cc-2c6f71b43900",
   "metadata": {},
   "source": [
    "**convolutional neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a8dfb-c9c8-44eb-9d1e-d82468921ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.603093765Z",
     "start_time": "2024-01-05T14:08:14.598673603Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inputDimension,\n",
    "            hiddenDimensions,\n",
    "            outputDimension,\n",
    "            Nx,\n",
    "            activationFunction=torch.nn.Tanh(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        modules.append(\n",
    "            torch.nn.Conv1d(\n",
    "                inputDimension, hiddenDimensions[0], kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "        )\n",
    "        modules.append(activationFunction)\n",
    "        for i in range(len(hiddenDimensions) - 1):\n",
    "            modules.append(\n",
    "                torch.nn.Conv1d(\n",
    "                    hiddenDimensions[i],\n",
    "                    hiddenDimensions[i + 1],\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            modules.append(activationFunction)\n",
    "        modules.append(\n",
    "            torch.nn.Conv1d(\n",
    "                hiddenDimensions[-1],\n",
    "                outputDimension,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "\n",
    "        self.modelInput = torch.randn(\n",
    "            (1, inputDimension, Nx)\n",
    "        )  # Gaussian noise as input\n",
    "        self.modelInput = (\n",
    "                self.modelInput\n",
    "                / (torch.max(self.modelInput) - torch.min(self.modelInput))\n",
    "                * 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(self.modelInput).reshape(\n",
    "            -1, 1\n",
    "        )  # x is a dummy variable to match the interface of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997a44b-2089-42b8-b3cc-dcb75599c2d7",
   "metadata": {},
   "source": [
    "**initialization of neural network weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccef84d-eadf-4dc1-8298-b1062faa5f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.603591203Z",
     "start_time": "2024-01-05T14:08:14.598715665Z"
    }
   },
   "outputs": [],
   "source": [
    "def initWeights(m):\n",
    "    \"\"\"Initialize weights of neural network with xavier initialization.\"\"\"\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            m.weight, gain=torch.nn.init.calculate_gain(\"tanh\")\n",
    "        )  # adapt if using a different initialization in block 10\n",
    "        # torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        # torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain('sigmoid'))\n",
    "        m.bias.data.fill_(0.0)\n",
    "    if type(m) == torch.nn.Conv1d:\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            m.weight, gain=torch.nn.init.calculate_gain(\"leaky_relu\", 0.2)\n",
    "        )\n",
    "        m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac280a6c-0742-4bf8-b6f2-0995b3680079",
   "metadata": {},
   "source": [
    "## PINN helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551c1ad-ef82-4af5-bc7c-658fb6d16913",
   "metadata": {},
   "source": [
    "**displacement computation**\n",
    "$$\\hat{u}=F_{NN}(x)$$\n",
    "or\n",
    "$$\\hat{u}=F_{NN}(x)\\cdot x\\cdot(1-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eda658-51ba-47bc-a7ff-ed2bb47c4c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.604090364Z",
     "start_time": "2024-01-05T14:08:14.598751942Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDisplacements(model, x):\n",
    "    return model(x)\n",
    "\n",
    "#    return model(x) * x * (1 - x) # satisfies the boundary conditions by construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9999f25-b7e7-42fa-a1bd-4824a0a32fa3",
   "metadata": {},
   "source": [
    "**loss term computation**\n",
    "\n",
    "the differential equation loss\n",
    "$$\\mathcal{L}_R=\\sum_{i=1}^N\\bigl(\\frac{d}{dx}EA\\bigl(\\frac{d\\hat{u}}{dx}\\bigr)+p\\bigr)^2$$\n",
    "the boundary condition loss \n",
    "$$\\mathcal{L}_B=\\sum_{i=1}^{N_B}\\bigl( \\frac{d^{n_i} \\hat{u}}{dx^{n_i}} - F \\bigr)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e36d4-ee7e-4450-8c50-a4ba4fd101e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.604568178Z",
     "start_time": "2024-01-05T14:08:14.598866518Z"
    }
   },
   "outputs": [],
   "source": [
    "def getLossTerms(x, xB, u, uB, EA, distLoad, uBLabel, weights):\n",
    "    differentialEquationLoss = (\n",
    "            getDerivative(EA * getDerivative(u, x, 1), x, 1) + distLoad\n",
    "    )\n",
    "    differentialEquationLoss = torch.sum(differentialEquationLoss ** 2).squeeze()\n",
    "    #    differentialEquationLoss = torch.sum(differentialEquationLoss ** 2 * weights[len(uBLabel):]).squeeze() # for automatic weighting of all collocation points, remember to also modify block 11\n",
    "\n",
    "    # initialization\n",
    "    boundaryConditionLoss = 0\n",
    "\n",
    "    for i in range(len(uBLabel)):\n",
    "        boundaryConditionLoss += (\n",
    "                                         getDerivative(uB, xB, uBLabel[i][1])[i] - uBLabel[i][0]\n",
    "                                 ).squeeze() ** 2\n",
    "    #        boundaryConditionLoss += (getDerivative(uB, xB, uBLabel[i][1])[i] - uBLabel[i][0]).squeeze() ** 2 * weights[i] # for automatic weighting of all collocation points, remember to also modify block 11\n",
    "\n",
    "    return differentialEquationLoss, boundaryConditionLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48beb-255a-4042-b29b-bdf06a4cdd3a",
   "metadata": {},
   "source": [
    "**cost function computation**\n",
    "$$C=\\mathcal{L}_R+\\mathcal{L}_B$$\n",
    "or \n",
    "$$C=\\kappa_R\\mathcal{L}_R+\\kappa_B\\mathcal{L}_B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbab3de-4767-45c9-a4d3-6bfb638d243a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.605048777Z",
     "start_time": "2024-01-05T14:08:14.598906398Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCostFunction(lossTerms, weights):\n",
    "    return lossTerms[0] + lossTerms[1]\n",
    "\n",
    "#    return lossTerms[0] + lossTerms[1] * 1e3 # manual weighting\n",
    "#    return lossTerms[0] * weights[0] + lossTerms[1] * weights[1] # automatic weighting, remember to also modify block 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e7265-83d4-4be6-b5d2-018649c32b21",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957569f-3de2-43dd-9b11-3221bf3d04b9",
   "metadata": {},
   "source": [
    "**physical parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7459465-221f-43d6-8c9f-0d7ff6938496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.605521051Z",
     "start_time": "2024-01-05T14:08:14.598940653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analytial solution\n",
    "uAnalytic = lambda x: np.sin(2 * np.pi * x)  # (1. - np.cos(3. * np.pi * x))\n",
    "\n",
    "# Problem data\n",
    "E = lambda x: 1.0 + x * 0  # Young's modulus\n",
    "A = lambda x: 1.0 + x * 0  # cross-sectional area\n",
    "L = 1.0  # bar length\n",
    "uB = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, L],\n",
    "]  # boundary conditions: [value, degree of differentiation, index]\n",
    "distLoad = lambda x: 4 * np.pi ** 2 * torch.sin(2 * np.pi * x)  # distributed load p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f533b6-272e-460a-8355-0850025d400f",
   "metadata": {},
   "source": [
    "**hyperparameters**\n",
    "\n",
    "currently Adam is selected as optimizer. By commenting the Adam block and uncommenting the L-BFGS block, you can enable L-BFGS as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15917078-13cb-4725-9164-cf409ada9530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.605989415Z",
     "start_time": "2024-01-05T14:08:14.598982791Z"
    }
   },
   "outputs": [],
   "source": [
    "Nx = 100  # number of collocation points\n",
    "hiddenDimensions = [100]  # definition of hidden layers\n",
    "activationFunction = (\n",
    "    torch.nn.Tanh()\n",
    ")  # if this is changed, also adapt the initialization in block 5\n",
    "\n",
    "# activationFunction = torch.nn.ReLU()\n",
    "# activationFunction = torch.nn.Sigmoid()\n",
    "\n",
    "# activationFunction = makeAdaptiveActivation(10, torch.nn.Tanh()) # hyperparameter n=10 controls the learning rate of the activation\n",
    "\n",
    "alpha = -0.5\n",
    "beta = 0.2\n",
    "initialWeights = 1e0  # emulates learning rates for weighting terms, could be modified with (optimizer.param_groups[-1]['lr'] = lr * lrWeights), but interferes with scheduler\n",
    "\n",
    "epochs = 5000  # number of epochs\n",
    "lr = 1e-3  # learning rate (if learning rate scheduler active, increase to lr = 5e-2)\n",
    "selectOptimizer = \"Adam\"\n",
    "\n",
    "# epochs = 500\n",
    "# selectOptimizer = \"LBFGS\"\n",
    "# lr = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309db2e-aa72-4e72-bbd5-0d1df04fa0bf",
   "metadata": {},
   "source": [
    "**neural network & optimizer setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66f248-a5c1-4ca7-8552-41ffbe2e7828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.667736801Z",
     "start_time": "2024-01-05T14:08:14.599021337Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FNN(1, hiddenDimensions, 1, activationFunction)\n",
    "\n",
    "# hiddenDimensions = [20, 40, 20, 10] # adapted hyperparameters for CNN\n",
    "# lr = 2e-3\n",
    "# activationFunction = torch.nn.PReLU(init=0.2)\n",
    "# model = CNN(10, hiddenDimensions, 1, Nx, activationFunction)\n",
    "\n",
    "model.apply(initWeights)\n",
    "if selectOptimizer == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "elif selectOptimizer == \"LBFGS\":\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_lambda = lambda epoch: (beta * epoch + 1) ** alpha\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da81f0-95f1-46ca-85ae-ff17ee951e46",
   "metadata": {},
   "source": [
    "**additional learnable parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d71ed1-f6b3-4b09-970c-64fc05bcf6a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.668437682Z",
     "start_time": "2024-01-05T14:08:14.642623520Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = torch.tensor([])\n",
    "# weights = torch.tensor([1, 1]) # for automatic weighting of loss terms, remember to also modify block 7\n",
    "# weights = torch.ones(Nx + len(uB)) # for automatic weighting of all collocation points, remember to also modify block 6\n",
    "\n",
    "weights *= initialWeights\n",
    "weights.requires_grad = True\n",
    "# optimizer.add_param_group({'params': weights}) # add weights to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fddfc-395b-4339-bebe-6558fde75679",
   "metadata": {},
   "source": [
    "**training grid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8609cae-76eb-450e-9b84-75a69068c7a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:14.668986689Z",
     "start_time": "2024-01-05T14:08:14.642740137Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0, L, Nx, requires_grad=True).unsqueeze(\n",
    "    1\n",
    ")  # sampling: uniform spacing\n",
    "\n",
    "# sampler = qmc.LatinHypercube(d=1) # sampling: latin hypercube\n",
    "# x = torch.from_numpy(sampler.random(Nx - 2) * L).to(torch.float32)\n",
    "# x = torch.sort(x)\n",
    "# x = torch.cat((torch.tensor([[0]]), x, torch.tensor([[L]]))) # to include the boundary points\n",
    "# x.requires_grad = True\n",
    "\n",
    "# x = torch.from_numpy((0.5*np.polynomial.legendre.leggauss(Nx - 2)[0]+0.5) * L).to(torch.float32).unsqueeze(1) # sampling: Gauss-Legendre points + boundary points\n",
    "# x = torch.cat((torch.tensor([[0]]), x, torch.tensor([[L]]))) # to include the boundary points\n",
    "# x.requires_grad = True\n",
    "\n",
    "# boundary points\n",
    "xB = torch.tensor([uBi[2] for uBi in uB]).unsqueeze(1).to(torch.float32)\n",
    "xB.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f755e-e923-49d3-954c-bf09cd8ee74d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bca03-41d4-4a84-9945-326f7f59c3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:20.695574836Z",
     "start_time": "2024-01-05T14:08:14.642787251Z"
    }
   },
   "outputs": [],
   "source": [
    "differentialEquationLossHistory = np.zeros(epochs)\n",
    "boundaryConditionLossHistory = np.zeros(epochs)\n",
    "costHistory = np.zeros(epochs)\n",
    "\n",
    "start = time.perf_counter()\n",
    "start0 = start\n",
    "for epoch in range(epochs):\n",
    "    # predict displacements\n",
    "    uPred = getDisplacements(model, x)\n",
    "    uBPred = getDisplacements(model, xB)\n",
    "\n",
    "    lossTerms = getLossTerms(\n",
    "        x, xB, uPred, uBPred, E(x) * A(x), distLoad(x), uB, weights\n",
    "    )\n",
    "    differentialEquationLossHistory[epoch] = lossTerms[0].detach()\n",
    "    boundaryConditionLossHistory[epoch] = lossTerms[1].detach()\n",
    "    costHistory[epoch] = getCostFunction(lossTerms, weights).detach()\n",
    "\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        uPred = getDisplacements(model, x)\n",
    "        uBPred = getDisplacements(model, xB)\n",
    "        lossTerms = getLossTerms(\n",
    "            x, xB, uPred, uBPred, E(x) * A(x), distLoad(x), uB, weights\n",
    "        )\n",
    "        cost = getCostFunction(lossTerms, weights)\n",
    "        cost.backward()\n",
    "        if weights.grad != None:\n",
    "            weights.grad = -weights.grad  # maximization with regard to weighting terms\n",
    "        return cost\n",
    "\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    # scheduler.step() # learning rate scheduler\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        elapsedTime = (time.perf_counter() - start) / 100\n",
    "        string = \"Epoch: {}/{}\\t\\tDifferential equation loss = {:2e}\\t\\tBoundary condition closs = {:2e}\\nCost = {:2e}\\t\\tElapsed time = {:2f}\"\n",
    "        # Format string and print\n",
    "        print(\n",
    "            string.format(\n",
    "                epoch,\n",
    "                epochs - 1,\n",
    "                differentialEquationLossHistory[epoch],\n",
    "                boundaryConditionLossHistory[epoch],\n",
    "                costHistory[epoch],\n",
    "                elapsedTime,\n",
    "            )\n",
    "        )\n",
    "        start = time.perf_counter()\n",
    "elapsedTime = time.perf_counter() - start0\n",
    "string = \"Total elapsed time: {:2f}\\nAverage elapsed time per epoch: {:2f}\"\n",
    "print(string.format(elapsedTime, elapsedTime / epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e63a9d-ea31-4930-90bb-b429637ca35c",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b677c7-9d31-4451-8562-8aa347e04726",
   "metadata": {},
   "source": [
    "**training history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6c235-0210-40aa-a4d4-b93afc51989f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:20.980598965Z",
     "start_time": "2024-01-05T14:08:20.701103036Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"cost function $C$\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.plot(costHistory, \"k\", linewidth=2, label=\"cost $C$\")\n",
    "ax.plot(\n",
    "    differentialEquationLossHistory,\n",
    "    \"r:\",\n",
    "    linewidth=2,\n",
    "    label=\"differential equation loss $\\\\mathcal{L}_{\\\\mathcal{R}}$\",\n",
    ")\n",
    "ax.plot(\n",
    "    boundaryConditionLossHistory,\n",
    "    \"b--\",\n",
    "    linewidth=2,\n",
    "    label=\"boundary condition loss $\\\\mathcal{L}_{\\\\mathcal{B}}$\",\n",
    ")\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f6de0-2ef2-4c1d-bad3-714c954ac14f",
   "metadata": {},
   "source": [
    "**displacement prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd55fa0-2d9e-4e79-be43-3f4164db12ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:21.088404875Z",
     "start_time": "2024-01-05T14:08:20.983362343Z"
    }
   },
   "outputs": [],
   "source": [
    "xTest = torch.linspace(0, L, 1000).unsqueeze(1)\n",
    "uPredTest = getDisplacements(model, xTest).detach()  # disable with CNN\n",
    "uPred = getDisplacements(model, x).detach()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"Displacement $u$\")\n",
    "\n",
    "ax.plot(xTest, uAnalytic(xTest), \"gray\", linewidth=2, label=\"Analytical solution\")\n",
    "ax.plot(xTest, uPredTest, \"k:\", linewidth=2, label=\"Prediction\")  # disable with CNN\n",
    "ax.plot(x.detach(), uPred, \"rs\", markersize=6, label=\"Collocation points\")\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "# plt.savefig(\"prediction.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b16985-dabf-4472-a91a-4e603c6eaf69",
   "metadata": {},
   "source": [
    "**L2 norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a94f8-6d13-4e29-b7ba-24bf2d7d07b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:21.091910329Z",
     "start_time": "2024-01-05T14:08:21.089119355Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"{:2e}\".format(\n",
    "        (\n",
    "                1.0\n",
    "                / L\n",
    "                * np.sqrt(\n",
    "            np.trapz(\n",
    "                (uPred[:, 0] - uAnalytic(x.detach())[:, 0]) ** 2,\n",
    "                dx=xTest[1] - xTest[0],\n",
    "            )\n",
    "        )\n",
    "        ).item()\n",
    "    )\n",
    ")  # if CNN is used, x has to come from uniform grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4c780-b472-400e-bb1c-37350b527a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T14:08:21.145590616Z",
     "start_time": "2024-01-05T14:08:21.092318122Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"{:2e}\".format(\n",
    "        (\n",
    "                1.0\n",
    "                / L\n",
    "                * np.sqrt(\n",
    "            np.trapz(\n",
    "                (uPredTest[:, 0] - uAnalytic(xTest)[:, 0]) ** 2,\n",
    "                dx=xTest[1] - xTest[0],\n",
    "            )\n",
    "        )\n",
    "        ).item()\n",
    "    )\n",
    ")  # if CNN is used this does not work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
